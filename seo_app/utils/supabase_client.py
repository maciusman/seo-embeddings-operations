import streamlit as st # For error/info messages, though ideally keep this minimal in backend logic
from supabase import create_client, Client
from supabase.lib.client_options import ClientOptions
from postgrest.exceptions import APIError
import logging
from datetime import datetime # Added for type hinting in __main__ example if used

logger = logging.getLogger(__name__)
# logging.basicConfig(level=logging.INFO) # Configure if running standalone

# --- Global Supabase Client ---
# This is not ideal for long-running apps if keys change, but simple for Streamlit's execution model.
# A better approach might be to pass the client around or use st.session_state.
_supabase_client: Client = None

def init_supabase_client(url: str, key: str) -> Client | None:
    """Initializes the Supabase client and stores it globally."""
    global _supabase_client
    if not url or not key:
        logger.error("Supabase URL and Key are required.")
        # st.error("Supabase URL and Key are required.") # Avoid direct st calls in backend logic
        _supabase_client = None
        return None
    try:
        logger.info(f"Initializing Supabase client with URL: {url[:20]}...") # Log only part of URL
        _supabase_client = create_client(url, key)
        logger.info("Supabase client initialized successfully.")
        return _supabase_client
    except Exception as e:
        logger.error(f"Failed to initialize Supabase client: {e}", exc_info=True)
        # st.error(f"Failed to initialize Supabase client: {e}")
        _supabase_client = None
        return None

def get_supabase_client() -> Client | None:
    """Returns the initialized Supabase client."""
    global _supabase_client
    if _supabase_client is None:
        logger.warning("Supabase client not initialized. Call init_supabase_client first.")
        if 'supabase_client' in st.session_state and st.session_state.supabase_client: # Check Streamlit session state
            _supabase_client = st.session_state.supabase_client
            return _supabase_client
        # st.error("Supabase client is not initialized. Please connect in the app settings.") # Avoid direct st calls
    return _supabase_client

def ensure_tables_exist(client: Client) -> bool:
    """
    Checks if 'crawled_pages' and 'page_embeddings' tables exist, and creates them if not.
    Uses Supabase RPC for DDL commands.
    Returns True if tables are ensured, False otherwise.
    Streamlit calls (st.error, st.warning, st.success) are now passed to logger or returned.
    """
    if not client:
        logger.error("Supabase client not available for table creation.")
        return False # Indicate failure

    EMBEDDING_DIM = 768

    crawled_pages_schema = f"""
    CREATE TABLE IF NOT EXISTS public.crawled_pages (
        id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
        url TEXT UNIQUE NOT NULL,
        domain TEXT,
        crawl_timestamp TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        h1_headings TEXT,
        title TEXT,
        meta_description TEXT,
        main_content_raw TEXT,
        keywords TEXT,
        links_internal TEXT[],
        links_external TEXT[],
        word_count INTEGER,
        char_count INTEGER,
        sentiment_score FLOAT,
        last_modified_at TIMESTAMP WITH TIME ZONE,
        etag TEXT,
        status_code INTEGER,
        content_type TEXT,
        raw_html_preview TEXT,
        crawled_by TEXT
    );
    CREATE INDEX IF NOT EXISTS idx_crawled_pages_url ON public.crawled_pages(url);
    CREATE INDEX IF NOT EXISTS idx_crawled_pages_domain ON public.crawled_pages(domain);
    """

    page_embeddings_schema = f"""
    CREATE TABLE IF NOT EXISTS public.page_embeddings (
        id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
        page_id BIGINT NOT NULL REFERENCES public.crawled_pages(id) ON DELETE CASCADE,
        embedding_type TEXT NOT NULL,
        embedding_vector vector({EMBEDDING_DIM}),
        model_name TEXT NOT NULL,
        task_type TEXT,
        generated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        token_count INTEGER,
        UNIQUE (page_id, embedding_type, model_name, task_type)
    );
    CREATE INDEX IF NOT EXISTS idx_page_embeddings_page_id ON public.page_embeddings(page_id);
    """

    # Ensure pgvector extension
    try:
        # Using `rpc('sql', ...)` is not a standard Supabase function.
        # The correct way to execute arbitrary SQL is often via a custom RPC function like `exec`
        # or by ensuring the extension is enabled via the Supabase Dashboard.
        # For this script, we'll log the intent and assume it might need manual setup if `exec` RPC isn't there.
        logger.info("Attempting to ensure pgvector extension exists using 'CREATE EXTENSION IF NOT EXISTS vector'.")
        # This specific RPC call might fail if 'sql' function doesn't exist or has wrong permissions.
        # A common workaround is to create a function `exec(sql_query text)` in Supabase SQL editor.
        # Example: CREATE OR REPLACE FUNCTION exec(sql_query text) RETURNS void AS $$ BEGIN EXECUTE sql_query; END; $$ LANGUAGE plpgsql;
        client.rpc('exec', params={'sql_query': "CREATE EXTENSION IF NOT EXISTS vector SCHEMA public;"}).execute()
        logger.info("pgvector extension ensured (or creation attempted).")
    except Exception as e:
        logger.warning(f"Could not automatically ensure pgvector extension via RPC 'exec': {e}. "
                       "This might be due to the 'exec' RPC function not existing or permissions issues. "
                       "Please ensure the pgvector extension is enabled in your Supabase DB (Database -> Extensions).")
        # We don't return False here, as table creation might still succeed if extension was already enabled.

    # Ensure tables
    schemas_to_run = {
        "crawled_pages": crawled_pages_schema,
        "page_embeddings": page_embeddings_schema
    }
    all_successful = True
    for table_name, schema_sql in schemas_to_run.items():
        try:
            # Check if table exists by trying a simple select
            client.table(table_name).select("id").limit(1).execute()
            logger.info(f"Table '{table_name}' already exists.")
        except APIError as e:
            if e.code == '42P01': # undefined_table
                logger.info(f"Table '{table_name}' not found, attempting to create...")
                try:
                    for stmt in schema_sql.split(';'):
                        if stmt.strip():
                            client.rpc('exec', params={'sql_query': stmt}).execute() # Requires 'exec' RPC
                    logger.info(f"Table '{table_name}' schema execution attempted successfully.")
                except Exception as creation_e:
                    logger.error(f"Failed to create table '{table_name}': {creation_e}. "
                                 "This might be due to the 'exec' RPC function not existing or permissions. "
                                 "Please create tables manually via Supabase Studio if needed.")
                    all_successful = False # Mark as failed
            else: # Other API error
                logger.error(f"APIError checking table '{table_name}': {e.message}")
                all_successful = False
        except Exception as e_check: # Other unexpected error during check
            logger.error(f"Unexpected error checking table '{table_name}': {e_check}")
            all_successful = False

    if all_successful:
        logger.info("All tables ensured or creation attempted.")
    else:
        logger.warning("One or more tables could not be ensured automatically. Manual check/creation might be needed.")
    return all_successful


def get_page_by_url(client: Client, url: str) -> dict | None:
    """Retrieves a page by URL to get its ID and other data."""
    if not client: return None
    try:
        response = client.table("crawled_pages").select("id, url, title, domain").eq("url", url).limit(1).execute()
        if response.data:
            logger.debug(f"Page found for URL '{url}': ID {response.data[0]['id']}")
            return response.data[0]
        return None
    except Exception as e:
        logger.error(f"Error getting page by URL '{url}': {e}", exc_info=True)
        return None

def insert_crawled_page(client: Client, page_data: dict) -> int | None:
    """
    Inserts/Updates a single page's data into crawled_pages using upsert.
    Returns the page ID.
    """
    if not client: return None
    if 'url' not in page_data or not page_data['url']:
        logger.error("URL is missing in page_data, cannot insert.")
        return None

    db_page_data = {k: v for k, v in page_data.items() if v is not None}
    try:
        logger.debug(f"Upserting page data for URL: {db_page_data['url']}")
        response = client.table("crawled_pages").upsert(db_page_data, on_conflict="url").execute()

        if response.data:
            page_id = response.data[0]['id']
            logger.debug(f"Page data upserted for URL '{db_page_data['url']}', ID: {page_id}")
            return page_id
        else: # Fallback if upsert doesn't return data (should with default 'representation')
            fetched_page = get_page_by_url(client, db_page_data['url'])
            if fetched_page: return fetched_page['id']
            logger.error(f"Failed to get ID after upserting page URL '{db_page_data['url']}'. Response: {response}")
            return None
    except Exception as e:
        logger.error(f"Error upserting page data for URL '{page_data.get('url')}': {e}", exc_info=True)
        return None

def get_embeddings_for_page(client: Client, page_id: int, embedding_type: str, model_name: str, task_type: str) -> list | None:
    """Retrieves existing embeddings for a page, type, model, and task."""
    if not client: return None
    try:
        response = client.table("page_embeddings").select("id, embedding_vector") \
            .eq("page_id", page_id) \
            .eq("embedding_type", embedding_type) \
            .eq("model_name", model_name) \
            .eq("task_type", task_type) \
            .execute()
        if response.data:
            logger.debug(f"Found {len(response.data)} existing embeddings for page_id {page_id}, type '{embedding_type}'.")
            return response.data
        return None
    except Exception as e:
        logger.error(f"Error getting embeddings for page_id {page_id}, type '{embedding_type}': {e}", exc_info=True)
        return None

def insert_page_embedding(client: Client, page_id: int, embedding_type: str, embedding_vector: list, model_name: str, task_type: str, token_count: int = None) -> int | None:
    """
    Inserts/Updates an embedding into page_embeddings using upsert. Returns the embedding ID.
    """
    if not client: return None

    embedding_data = {
        "page_id": page_id, "embedding_type": embedding_type,
        "embedding_vector": embedding_vector, "model_name": model_name,
        "task_type": task_type, "token_count": token_count,
        "generated_at": "now()"
    }
    embedding_data = {k:v for k,v in embedding_data.items() if v is not None}

    try:
        conflict_cols = "page_id, embedding_type, model_name, task_type"
        logger.debug(f"Upserting embedding for page_id: {page_id}, type: {embedding_type}")
        response = client.table("page_embeddings").upsert(embedding_data, on_conflict=conflict_cols).execute()

        if response.data:
            embedding_id = response.data[0]['id']
            logger.debug(f"Embedding upserted for page_id {page_id}, type '{embedding_type}'. ID: {embedding_id}")
            return embedding_id
        else: # Fallback
            existing = get_embeddings_for_page(client, page_id, embedding_type, model_name, task_type)
            if existing: return existing[0]['id']
            logger.error(f"Failed to get ID after upserting embedding for page {page_id}, type {embedding_type}.")
            return None
    except Exception as e:
        logger.error(f"Error upserting embedding for page_id {page_id}, type '{embedding_type}': {e}", exc_info=True)
        return None

if __name__ == "__main__":
    logger.info("Supabase client logic defined. For testing, provide URL/Key and use a dedicated test script.")
    # Removed direct st.calls from this file to make it more backend-friendly.
    # UI feedback should be handled by the calling Streamlit app code.
    # The ensure_tables_exist function now logs warnings if automatic DDL via RPC 'exec' might fail,
    # guiding users to check Supabase Studio for table/extension setup.
```
